{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n\n# -----------------\n# Regression Losses\n# -----------------\ndef mse(y, y_pred):\n    y, y_pred = np.asarray(y), np.asarray(y_pred)\n    return np.mean((y - y_pred) ** 2)\n\ndef rmse(y, y_pred):\n    return np.sqrt(mse(y, y_pred))\n\ndef mae(y, y_pred):\n    y, y_pred = np.asarray(y), np.asarray(y_pred)\n    return np.mean(np.abs(y - y_pred))\n\ndef huber(y, y_pred, delta=1.0):\n    y, y_pred = np.asarray(y), np.asarray(y_pred)\n    r = y - y_pred\n    mask = np.abs(r) <= delta\n    return np.mean(0.5 * r[mask]**2 + delta*(np.abs(r[~mask]) - 0.5*delta))\n\ndef log_cosh(y, y_pred):\n    y, y_pred = np.asarray(y), np.asarray(y_pred)\n    return np.mean(np.log(np.cosh(y_pred - y)))\n\ndef quantile_loss(y, y_pred, q=0.9):\n    \"\"\"Pinball/Quantile loss: q in (0,1).\"\"\"\n    y, y_pred = np.asarray(y), np.asarray(y_pred)\n    e = y - y_pred\n    return np.mean(np.maximum(q*e, (q-1)*e))\n\n# --------------------\n# Classification Losses\n# --------------------\ndef binary_cross_entropy(y, p, eps=1e-12):\n    \"\"\"y in {0,1}; p is predicted probability of class 1.\"\"\"\n    y, p = np.asarray(y), np.clip(np.asarray(p), eps, 1-eps)\n    return np.mean(-(y*np.log(p) + (1-y)*np.log(1-p)))\n\ndef categorical_cross_entropy(y_onehot, p, eps=1e-12):\n    \"\"\"y_onehot: (n,k) one-hot targets; p: (n,k) predicted probs.\"\"\"\n    y_onehot, p = np.asarray(y_onehot), np.clip(np.asarray(p), eps, 1-eps)\n    return -np.mean(np.sum(y_onehot * np.log(p), axis=1))\n\ndef hinge(y, y_score):\n    \"\"\"y in {-1,+1}; y_score is raw signed score.\"\"\"\n    y, y_score = np.asarray(y), np.asarray(y_score)\n    return np.mean(np.maximum(0.0, 1 - y*y_score))\n\ndef squared_hinge(y, y_score):\n    y, y_score = np.asarray(y), np.asarray(y_score)\n    return np.mean(np.maximum(0.0, 1 - y*y_score) ** 2)\n\ndef focal_loss_binary(y, p, gamma=2.0, alpha=0.25, eps=1e-12):\n    \"\"\"Binary focal loss used for class imbalance.\"\"\"\n    y, p = np.asarray(y), np.clip(np.asarray(p), eps, 1-eps)\n    pt = np.where(y == 1, p, 1-p)\n    alpha_t = np.where(y == 1, alpha, 1-alpha)\n    return np.mean(-alpha_t * (1-pt)**gamma * np.log(pt))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:14:43.284978Z","iopub.execute_input":"2025-11-10T04:14:43.285216Z","iopub.status.idle":"2025-11-10T04:14:43.292676Z","shell.execute_reply.started":"2025-11-10T04:14:43.285197Z","shell.execute_reply":"2025-11-10T04:14:43.291929Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Regression demo\nrng = np.random.default_rng(0)\ny = rng.normal(0, 1, 10)\ny_pred = y + rng.normal(0, 0.5, 10)\n\nprint(\"MSE:\", mse(y, y_pred))\nprint(\"MAE:\", mae(y, y_pred))\nprint(\"Huber:\", huber(y, y_pred, delta=1.0))\nprint(\"LogCosh:\", log_cosh(y, y_pred))\nprint(\"Quantile(0.9):\", quantile_loss(y, y_pred, q=0.9))\n\n# Classification demo (binary)\ny_bin = rng.integers(0, 2, 12)\np_bin = np.clip(rng.uniform(0, 1, 12), 1e-6, 1-1e-6)\nprint(\"BCE:\", binary_cross_entropy(y_bin, p_bin))\nprint(\"Focal:\", focal_loss_binary(y_bin, p_bin, gamma=2, alpha=0.25))\n\n# Hinge needs labels in {-1, +1} and raw scores\ny_pm = np.where(y_bin==1, 1, -1)\nscores = rng.normal(0, 1, 12)\nprint(\"Hinge:\", hinge(y_pm, scores))\nprint(\"Squared Hinge:\", squared_hinge(y_pm, scores))\n\n# Multiclass cross-entropy\nn, k = 5, 3\ny_idx = rng.integers(0, k, n)\ny_onehot = np.eye(k)[y_idx]\nlogits = rng.normal(size=(n, k))\n# softmax for probs\np = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\nprint(\"Categorical CE:\", categorical_cross_entropy(y_onehot, p))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:15:12.674140Z","iopub.execute_input":"2025-11-10T04:15:12.674434Z","iopub.status.idle":"2025-11-10T04:15:12.685536Z","shell.execute_reply.started":"2025-11-10T04:15:12.674417Z","shell.execute_reply":"2025-11-10T04:15:12.684948Z"}},"outputs":[{"name":"stdout","text":"MSE: 0.2396215951994023\nMAE: 0.3750652113727884\nHuber: 0.7205583832216177\nLogCosh: 0.10650460025778982\nQuantile(0.9): 0.27773989482894745\nBCE: 1.7769408820901214\nFocal: 0.8760853689396217\nHinge: 1.3482071704651235\nSquared Hinge: 2.9116271759001258\nCategorical CE: 1.4788490256188567\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.metrics import log_loss, hinge_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(n_samples=1500, n_features=20, random_state=0)\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.25, random_state=0)\n\n# Probabilistic model\nlogreg = LogisticRegression(max_iter=500).fit(Xtr, ytr)\np = logreg.predict_proba(Xte)\nprint(\"Log Loss (BCE):\", log_loss(yte, p))\n\n# Large-margin model\nsgd_svm = SGDClassifier(loss=\"hinge\", random_state=0).fit(Xtr, ytr)\nmargin = sgd_svm.decision_function(Xte)\nprint(\"Hinge Loss:\", hinge_loss(yte*2-1, margin))  # convert to {-1,+1}\nprint(\"Accuracy:\", accuracy_score(yte, sgd_svm.predict(Xte)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T04:15:36.523639Z","iopub.execute_input":"2025-11-10T04:15:36.523981Z","iopub.status.idle":"2025-11-10T04:15:37.994184Z","shell.execute_reply.started":"2025-11-10T04:15:36.523960Z","shell.execute_reply":"2025-11-10T04:15:37.993468Z"}},"outputs":[{"name":"stdout","text":"Log Loss (BCE): 0.24481385579506365\nHinge Loss: 0.42566816628214665\nAccuracy: 0.8853333333333333\n","output_type":"stream"}],"execution_count":6}]}