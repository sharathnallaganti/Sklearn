{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Minimal GPT-style Transformer (decoder-only) in PyTorch\n# ------------------------------------------------------\n# - Learned token + positional embeddings\n# - Masked multi-head self-attention (causal)\n# - Feedforward (MLP) + residual connections + LayerNorm (pre-LN)\n# - Tied output head (weights shared with token embeddings)\n# - Greedy/top-k text generation helper\n#\n# NOTE: This is a compact demonstration We can extend for real training.\n\nimport math\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# -------------------------\n# Configuration\n# -------------------------\n@dataclass\nclass GPTConfig:\n    vocab_size: int = 256        # for a byte-level toy tokenizer\n    n_layer: int = 4\n    n_head: int = 4\n    n_embd: int = 256\n    block_size: int = 128        # max context length\n    dropout: float = 0.1\n\n\n# -------------------------\n# Building blocks\n# -------------------------\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0, \"n_embd must be divisible by n_head\"\n        self.n_head = config.n_head\n        self.head_dim = config.n_embd // config.n_head\n        self.scale = 1.0 / math.sqrt(self.head_dim)\n\n        # Projections\n        self.q_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        self.k_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        self.v_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n        self.out_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n\n        self.attn_drop = nn.Dropout(config.dropout)\n        self.resid_drop = nn.Dropout(config.dropout)\n\n        # Causal mask as a buffer (1 for allowed, 0 for masked)\n        mask = torch.tril(torch.ones(config.block_size, config.block_size))\n        # Shape to [1, 1, T, T] so it can broadcast across batch & heads\n        self.register_buffer(\"causal_mask\", mask.view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, T, C = x.shape\n\n        q = self.q_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hs)\n        k = self.k_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hs)\n        v = self.v_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, nh, T, hs)\n\n        # Scaled dot-product attention\n        att = (q @ k.transpose(-2, -1)) * self.scale  # (B, nh, T, T)\n\n        # Apply causal mask: only attend to <= current position\n        att = att.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float(\"-inf\"))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_drop(att)\n\n        y = att @ v  # (B, nh, T, hs)\n        y = y.transpose(1, 2).contiguous().view(B, T, C)  # concat heads\n        y = self.resid_drop(self.out_proj(y))\n        return y\n\n\nclass MLP(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        self.fc1 = nn.Linear(config.n_embd, 4 * config.n_embd)\n        self.fc2 = nn.Linear(4 * config.n_embd, config.n_embd)\n        self.drop = nn.Dropout(config.dropout)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.fc1(x)\n        x = F.gelu(x)\n        x = self.drop(self.fc2(x))\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln2 = nn.LayerNorm(config.n_embd)\n        self.mlp = MLP(config)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Pre-LN residual block\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\n# -------------------------\n# The mini GPT model\n# -------------------------\nclass MiniGPT(nn.Module):\n    def __init__(self, config: GPTConfig):\n        super().__init__()\n        self.config = config\n\n        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n        self.pos_emb = nn.Embedding(config.block_size, config.n_embd)\n        self.drop = nn.Dropout(config.dropout)\n\n        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n        self.ln_f = nn.LayerNorm(config.n_embd)\n\n        # Language modeling head (tied with token embeddings)\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.lm_head.weight = self.tok_emb.weight  # weight tying\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                nn.init.zeros_(module.bias)\n        if isinstance(module, nn.LayerNorm):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def forward(self, idx: torch.Tensor, targets: Optional[torch.Tensor] = None):\n        \"\"\"\n        idx: (B, T) int token ids\n        targets: (B, T) next-token ids for LM loss (optional)\n        returns: logits (B, T, vocab_size), and optional loss\n        \"\"\"\n        B, T = idx.shape\n        if T > self.config.block_size:\n            raise ValueError(f\"Sequence length {T} > block_size {self.config.block_size}\")\n\n        # embeddings\n        tok = self.tok_emb(idx)                                   # (B, T, C)\n        pos = self.pos_emb(torch.arange(T, device=self.device))   # (T, C)\n        x = self.drop(tok + pos.unsqueeze(0))                     # (B, T, C)\n\n        # transformer blocks\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.ln_f(x)\n        logits = self.lm_head(x)  # (B, T, vocab)\n\n        loss = None\n        if targets is not None:\n            loss = F.cross_entropy(logits.view(B * T, -1), targets.view(B * T))\n        return logits, loss\n\n    @torch.no_grad()\n    def generate(\n        self,\n        idx: torch.Tensor,\n        max_new_tokens: int,\n        temperature: float = 1.0,\n        top_k: Optional[int] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Autoregressive generation. idx is (B, T) with existing context.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # Crop to the last block_size tokens\n            idx_cond = idx[:, -self.config.block_size:]\n\n            logits, _ = self(idx_cond)                # (B, T, vocab)\n            logits = logits[:, -1, :] / max(1e-6, temperature)  # (B, vocab)\n\n            if top_k is not None:\n                # Top-k filtering\n                v, _ = torch.topk(logits, k=top_k, dim=-1)\n                thresh = v[:, [-1]]\n                logits = torch.where(logits < thresh, torch.full_like(logits, float(\"-inf\")), logits)\n\n            probs = F.softmax(logits, dim=-1)\n            next_token = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            idx = torch.cat([idx, next_token], dim=1)             # (B, T+1)\n        return idx\n\n\n# -------------------------\n# Tiny demo (toy char-level)\n# -------------------------\nif __name__ == \"__main__\":\n    # Device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Toy dataset (byte-level over a short string, for shape-check & sanity)\n    text = (\n        \"To build a tiny Transformer, we need embeddings, attention, feedforward layers, \"\n        \"residuals, and layer norms. This demo is small but faithful to the core ideas.\"\n    )\n    data = torch.tensor(list(text.encode(\"utf-8\")), dtype=torch.long)\n\n    # Train/val split\n    n = int(0.9 * len(data))\n    train_data, val_data = data[:n], data[n:]\n\n    cfg = GPTConfig(\n        vocab_size=256,  # byte-level\n        n_layer=4,\n        n_head=4,\n        n_embd=256,\n        block_size=64,\n        dropout=0.1,\n    )\n    model = MiniGPT(cfg).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n\n    def get_batch(split: str, batch_size: int = 32):\n        source = train_data if split == \"train\" else val_data\n        ix = torch.randint(0, len(source) - cfg.block_size - 1, (batch_size,))\n        x = torch.stack([source[i : i + cfg.block_size] for i in ix])\n        y = torch.stack([source[i + 1 : i + cfg.block_size + 1] for i in ix])\n        return x.to(device), y.to(device)\n\n    # A few quick training steps (for demonstration only)\n    model.train()\n    for step in range(200):  # increase for better learning\n        xb, yb = get_batch(\"train\")\n        logits, loss = model(xb, yb)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        if (step + 1) % 50 == 0:\n            with torch.no_grad():\n                xval, yval = get_batch(\"val\")\n                _, vloss = model(xval, yval)\n            print(f\"step {step+1:4d} | train loss {loss.item():.4f} | val loss {vloss.item():.4f}\")\n\n    # Generate a few bytes of text from a short prompt\n    model.eval()\n    prompt = b\"Transformers are\"\n    idx0 = torch.tensor([list(prompt)], dtype=torch.long, device=device)\n    out = model.generate(idx0, max_new_tokens=100, temperature=0.9, top_k=50)\n    generated = bytes(out[0].tolist()).decode(\"utf-8\", errors=\"ignore\")\n    print(\"\\n--- Generated sample ---\")\n    print(generated)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}